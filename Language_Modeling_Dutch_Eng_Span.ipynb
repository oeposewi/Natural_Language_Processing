{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw5.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOYLqSr2A4Zz+7UuF33jTMX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"Ytn2IrHRyzfo","executionInfo":{"status":"ok","timestamp":1637284470416,"user_tz":300,"elapsed":204,"user":{"displayName":"Oliver Posewitz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17090326966670826339"}},"outputId":"aaaf97b3-894f-4d9c-ef18-c594d2f64dc1"},"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","import pandas as pd\n","import numpy as np\n","tf.__version__"],"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2.7.0'"]},"metadata":{},"execution_count":44}]},{"cell_type":"markdown","metadata":{"id":"RJozk_J1tdco"},"source":["# Dutch to Spanish\n","\n","## There is no Dutch to Spanish database available in manythings.org, so we will create a Dutch to English, English to Spanish model."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4QpEfguQzUpe","executionInfo":{"status":"ok","timestamp":1637284476380,"user_tz":300,"elapsed":5012,"user":{"displayName":"Oliver Posewitz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17090326966670826339"}},"outputId":"63924284-4776-4db8-833e-a90a8359a525"},"source":["# Download data\n","!curl -O http://www.manythings.org/anki/nld-eng.zip\n","!unzip nld-eng.zip\n","data_path = \"nld.txt\" # Path to the data txt file on disk."],"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 2059k  100 2059k    0     0  5597k      0 --:--:-- --:--:-- --:--:-- 5597k\n","Archive:  nld-eng.zip\n","replace _about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n","  inflating: _about.txt              \n","replace nld.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n","  inflating: nld.txt                 \n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iG0HpbYwzzh3","executionInfo":{"status":"ok","timestamp":1637284478588,"user_tz":300,"elapsed":200,"user":{"displayName":"Oliver Posewitz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17090326966670826339"}},"outputId":"ff8e22a6-6801-4057-f4d8-c32baa0f2606"},"source":["# Configuration\n","batch_size = 64\n","epochs = 15\n","latent_dim = 256\n","\n","# length of dataset\n","num_samples = 20000\n","\n","# Read in data\n","input_texts = []\n","target_texts = []\n","input_characters = set()\n","target_characters = set()\n","\n","# The *with* statement is a handy way of managing file I/O. It ensures that\n","# the file gets closed once we are done with it.\n","with open(data_path, \"r\", encoding=\"utf-8\") as f:\n","    lines = f.read().split(\"\\n\")\n","\n","for line in lines[: min(num_samples, len(lines) - 1)]:\n","    target_text, input_text, _ = line.split(\"\\t\")\n","    # We use \"tab\" as the \"start sequence\" character\n","    # for the targets, and \"\\n\" as \"end sequence\" character.\n","    target_text = \"\\t\" + target_text + \"\\n\"\n","    input_texts.append(input_text)\n","    target_texts.append(target_text)\n","    for char in input_text:\n","        if char not in input_characters:\n","            input_characters.add(char)\n","    for char in target_text:\n","        if char not in target_characters:\n","            target_characters.add(char)\n","f.close()\n","\n","len(input_texts), len(target_texts)\n"],"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(20000, 20000)"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cUXLuHHq1Dxp","executionInfo":{"status":"ok","timestamp":1637284480051,"user_tz":300,"elapsed":6,"user":{"displayName":"Oliver Posewitz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17090326966670826339"}},"outputId":"2e7df7fb-fb82-45c9-956e-f9fc55e345eb"},"source":["# Take a peek at the first few pairs of input and target texts\n","[ (i, t) for i, t in enumerate(zip(input_texts[:15], target_texts[:15]))]"],"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0, ('Lopen!', '\\tGo.\\n')),\n"," (1, ('Vooruit.', '\\tGo.\\n')),\n"," (2, ('Hoi.', '\\tHi.\\n')),\n"," (3, ('HÃ©!', '\\tHi.\\n')),\n"," (4, ('Hai!', '\\tHi.\\n')),\n"," (5, ('Ren!', '\\tRun!\\n')),\n"," (6, ('Vlucht.', '\\tRun!\\n')),\n"," (7, ('Ren!', '\\tRun.\\n')),\n"," (8, ('Vlucht.', '\\tRun.\\n')),\n"," (9, ('Wie?', '\\tWho?\\n')),\n"," (10, (\"Da's niet gek!\", '\\tWow!\\n')),\n"," (11, ('Eend!', '\\tDuck!\\n')),\n"," (12, ('Vuur!', '\\tFire!\\n')),\n"," (13, ('Brand!', '\\tFire!\\n')),\n"," (14, ('Help!', '\\tHelp!\\n'))]"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-VbwL-5Z1KQ1","executionInfo":{"status":"ok","timestamp":1637284481667,"user_tz":300,"elapsed":220,"user":{"displayName":"Oliver Posewitz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17090326966670826339"}},"outputId":"f1ab41db-ca23-453f-a476-8d91aa5d3c7b"},"source":["# find the length of the maximum sequece for input and ouput\n","max_encoder_seq_length = max([len(txt) for txt in input_texts])\n","max_decoder_seq_length = max([len(txt) for txt in target_texts])\n","\n","print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n","print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n","\n","# sort list of unique characters for each input and get their lengths\n","input_characters = sorted(list(input_characters))\n","target_characters = sorted(list(target_characters))\n","num_encoder_tokens = len(input_characters)\n","num_decoder_tokens = len(target_characters)"],"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["Max sequence length for inputs: 52\n","Max sequence length for outputs: 24\n"]}]},{"cell_type":"markdown","metadata":{"id":"cmt3U_jQvdwW"},"source":["# Vectorize the input and target phrases"]},{"cell_type":"code","metadata":{"id":"xAOn_mUUzeRW","executionInfo":{"status":"ok","timestamp":1637284483688,"user_tz":300,"elapsed":499,"user":{"displayName":"Oliver Posewitz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17090326966670826339"}}},"source":["# Make Python dictionaries for the input and target messages\n","input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n","target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n","\n","# The encoder inputs - will hold character sequences for Dutch phrases\n","encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\")\n","\n","# The decoder inputs - will hold character sequences for English phrases\n","decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n","\n","# Same size numpy array for the decoder targets\n","decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")"],"execution_count":49,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jAvA0_GZvoly"},"source":["# Replace the characters in the input and output phrases with the vectors"]},{"cell_type":"code","metadata":{"id":"hPabnS8n3JBV","executionInfo":{"status":"ok","timestamp":1637284485964,"user_tz":300,"elapsed":751,"user":{"displayName":"Oliver Posewitz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17090326966670826339"}}},"source":["# Iterate over all of our phrase pairs\n","for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n","    \n","    # Iterate over all of the characters in the input phrase\n","    for t, char in enumerate(input_text):\n","        encoder_input_data[i, t, input_token_index[char]] = 1.0\n","\n","    # This adds padding with spaces\n","    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n","\n","    # Iterate over all of the characters in the target phrase. Here we are\n","    # filling two vectors \n","    for t, char in enumerate(target_text):\n","        # decoder_target_data is ahead of decoder_input_data by one timestep\n","        decoder_input_data[i, t, target_token_index[char]] = 1.0\n","        if t > 0:\n","            # decoder_target_data will be ahead by one timestep\n","            # and will not include the start character.\n","            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n","    \n","    # This adds padding with spaces\n","    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n","    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n"],"execution_count":50,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IzqUDQhOvtRI"},"source":["# Build the Encoder Model\n"]},{"cell_type":"code","metadata":{"id":"iMW9hJch3oA1","executionInfo":{"status":"ok","timestamp":1637284490369,"user_tz":300,"elapsed":990,"user":{"displayName":"Oliver Posewitz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17090326966670826339"}}},"source":["# 1. Define an input sequence and process it.\n","encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n","\n","# 2. Use a LSTM layer to process the input vectors. After today's lecture\n","# you should know what return_state does.\n","encoder = keras.layers.LSTM(latent_dim, return_state=True)\n","\n","# 3. Save the output from the encoder, but see step 4. \n","encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n","\n","# 4. We discard `encoder_outputs` and only keep the states.\n","encoder_states = [state_h, state_c]\n","\n","\n","\n","# This takes the target tokens as the input.\n","decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n","\n","# The LSTM later has the same internal dimensionality as for the encoder.\n","decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n","\n","# Save the decoder output: Note that this uses decoder_inputs\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n","\n","# Dense with softmax allows us to predict categorical output (our list of French characters)\n","decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n","\n","# Output layer\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# Define the overall model. This binds the encoder and decoder and will turn\n","# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n","model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"],"execution_count":51,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a9OKp2jZv16j"},"source":["# Compile the model "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0zHtN7uf4JiB","executionInfo":{"status":"ok","timestamp":1637285968172,"user_tz":300,"elapsed":231,"user":{"displayName":"Oliver Posewitz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17090326966670826339"}},"outputId":"da9f27b6-545d-44bc-f2dc-add49b6b4e81"},"source":["model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"categorical_accuracy\"])\n","\n","model.summary()"],"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_3\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_5 (InputLayer)           [(None, None, 81)]   0           []                               \n","                                                                                                  \n"," input_6 (InputLayer)           [(None, None, 76)]   0           []                               \n","                                                                                                  \n"," lstm_2 (LSTM)                  [(None, 256),        346112      ['input_5[0][0]']                \n","                                 (None, 256),                                                     \n","                                 (None, 256)]                                                     \n","                                                                                                  \n"," lstm_3 (LSTM)                  [(None, None, 256),  340992      ['input_6[0][0]',                \n","                                 (None, 256),                     'lstm_2[0][1]',                 \n","                                 (None, 256)]                     'lstm_2[0][2]']                 \n","                                                                                                  \n"," dense_1 (Dense)                (None, None, 76)     19532       ['lstm_3[0][0]']                 \n","                                                                                                  \n","==================================================================================================\n","Total params: 706,636\n","Trainable params: 706,636\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"JfkQTC_HwAga"},"source":["# Train the Dutch to English model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"10geanJ74ORJ","executionInfo":{"status":"ok","timestamp":1637284641240,"user_tz":300,"elapsed":145543,"user":{"displayName":"Oliver Posewitz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17090326966670826339"}},"outputId":"aaf190af-13cd-46a8-973e-8c3e6bdd3cce"},"source":["epochs = 10\n","\n","history = model.fit(\n","    [encoder_input_data, decoder_input_data],\n","    decoder_target_data,\n","    batch_size=batch_size,\n","    epochs=epochs,\n","    validation_split=0.2,)\n"],"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","250/250 [==============================] - 14s 43ms/step - loss: 2.0660 - categorical_accuracy: 0.4479 - val_loss: 2.2479 - val_categorical_accuracy: 0.3594\n","Epoch 2/10\n","250/250 [==============================] - 9s 34ms/step - loss: 1.4817 - categorical_accuracy: 0.5697 - val_loss: 1.9999 - val_categorical_accuracy: 0.4234\n","Epoch 3/10\n","250/250 [==============================] - 8s 34ms/step - loss: 1.2946 - categorical_accuracy: 0.6183 - val_loss: 1.7401 - val_categorical_accuracy: 0.4907\n","Epoch 4/10\n","250/250 [==============================] - 8s 34ms/step - loss: 1.1782 - categorical_accuracy: 0.6498 - val_loss: 1.6271 - val_categorical_accuracy: 0.5147\n","Epoch 5/10\n","250/250 [==============================] - 9s 34ms/step - loss: 1.0994 - categorical_accuracy: 0.6710 - val_loss: 1.5875 - val_categorical_accuracy: 0.5322\n","Epoch 6/10\n","250/250 [==============================] - 8s 34ms/step - loss: 1.0369 - categorical_accuracy: 0.6887 - val_loss: 1.5737 - val_categorical_accuracy: 0.5319\n","Epoch 7/10\n","250/250 [==============================] - 8s 34ms/step - loss: 0.9850 - categorical_accuracy: 0.7036 - val_loss: 1.5030 - val_categorical_accuracy: 0.5603\n","Epoch 8/10\n","250/250 [==============================] - 8s 34ms/step - loss: 0.9403 - categorical_accuracy: 0.7165 - val_loss: 1.4866 - val_categorical_accuracy: 0.5605\n","Epoch 9/10\n","250/250 [==============================] - 8s 34ms/step - loss: 0.9009 - categorical_accuracy: 0.7279 - val_loss: 1.3945 - val_categorical_accuracy: 0.5840\n","Epoch 10/10\n","250/250 [==============================] - 8s 34ms/step - loss: 0.8661 - categorical_accuracy: 0.7384 - val_loss: 1.3963 - val_categorical_accuracy: 0.5890\n"]}]},{"cell_type":"markdown","metadata":{"id":"oT-xUWhuwMLO"},"source":["# Construct the encoder and decoder"]},{"cell_type":"code","metadata":{"id":"oIWUr3k36IxT","executionInfo":{"status":"ok","timestamp":1637285999908,"user_tz":300,"elapsed":537,"user":{"displayName":"Oliver Posewitz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17090326966670826339"}}},"source":["# Define sampling models\n","\n","encoder_inputs = model.input[0]  # Encoder input layer\n","encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # First LSTM\n","\n","# This is the \"thought vector\" the hidden state that is used to start the decoder\n","encoder_states = [state_h_enc, state_c_enc]\n","encoder_model = keras.Model(encoder_inputs, encoder_states)\n","\n","decoder_inputs = model.input[1]  # This is the second/decoder input layer\n","decoder_state_input_h = keras.Input(shape=(latent_dim,))\n","decoder_state_input_c = keras.Input(shape=(latent_dim,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","\n","decoder_lstm = model.layers[3] # Second LSTM \n","\n","decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n","    decoder_inputs, initial_state=decoder_states_inputs\n",")\n","decoder_states = [state_h_dec, state_c_dec]\n","\n","decoder_dense = model.layers[4] # Dense output layer\n","\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","decoder_model = keras.Model(\n","    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",")"],"execution_count":56,"outputs":[]},{"cell_type":"code","metadata":{"id":"KxBfMOhrkp5S","executionInfo":{"status":"ok","timestamp":1637286006025,"user_tz":300,"elapsed":223,"user":{"displayName":"Oliver Posewitz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17090326966670826339"}}},"source":["# Make Python dictionaries that map vectors to characters\n","reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n","reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"],"execution_count":57,"outputs":[]},{"cell_type":"code","metadata":{"id":"9wdB759qobo_","executionInfo":{"status":"ok","timestamp":1637289350117,"user_tz":300,"elapsed":254,"user":{"displayName":"Oliver Posewitz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17090326966670826339"}}},"source":["# Decode sequences. convert a one-hot encoded input sequence\n","# and converts it into a \n","\n","def decode_sequence(input_seq):\n","    # Encode the input as state vectors.\n","    #start = time.process_time()\n","    states_value = encoder_model.predict(input_seq)\n","    #print(\"Encoding:\", time.process_time() - start)\n","\n","    # Generate empty target sequence of length 1.\n","    target_seq = np.zeros((1, 1, num_decoder_tokens))\n","    \n","    # Populate the first character of target sequence with the start character.\n","    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n","\n","    # Sampling loop for a batch of sequences\n","    stop_condition = False\n","    decoded_sentence = \"\"\n","    \n","    # Always a little risky to use a while loop, but we don't know what \n","    # length of sentence the decoder will issue - that's pretty much the \n","    # whole point of a sequence-to-sequence model, right?\n","    while not stop_condition:\n","        #start = time.process_time()\n","        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n","        #print(\"Decoding:\", time.process_time() - start)\n","        # Sample a token: Find the index of the most probable output character\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","\n","        # Use our reversing dictionary to decode the predicted character\n","        sampled_char = reverse_target_char_index[sampled_token_index]\n","        decoded_sentence += sampled_char\n","\n","        # Exit condition: either hit max length of a decoder output sequence\n","        # or find that the decoder has issued a stop character.\n","        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n","            stop_condition = True\n","\n","        # Update the target sequence (of length 1).\n","        target_seq = np.zeros((1, 1, num_decoder_tokens))\n","\n","        # This one-hot encodes the current character to use as input for the next iteration\n","        target_seq[0, 0, sampled_token_index] = 1.0\n","\n","        # Update states\n","        states_value = [h, c]\n","    \n","    return decoded_sentence"],"execution_count":68,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i7QW7nIgofhs","executionInfo":{"status":"ok","timestamp":1637290189036,"user_tz":300,"elapsed":27861,"user":{"displayName":"Oliver Posewitz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17090326966670826339"}},"outputId":"8748d7ba-4f26-43b5-f3d6-5552d330ca6b"},"source":["for seq_index in range(20):\n","    # Take one sequence (part of the training set)\n","    # for trying out decoding.\n","    input_seq = encoder_input_data[seq_index : seq_index + 1]\n","\n","    # Here's where we call our custom function\n","    decoded_sentence = decode_sequence(input_seq)\n","\n","    print(\"Input sentence:\", input_texts[seq_index], \"Decoded sentence:\", decoded_sentence)\n","    "],"execution_count":73,"outputs":[{"output_type":"stream","name":"stdout","text":["Input sentence: Lopen! Decoded sentence: Don't sell you sone?\n","\n","Input sentence: Vooruit. Decoded sentence: Let's see that book.\n","\n","Input sentence: Hoi. Decoded sentence: How did you stall?\n","\n","Input sentence: HÃ©! Decoded sentence: He was stall a lot.\n","\n","Input sentence: Hai! Decoded sentence: It's still to go.\n","\n","Input sentence: Ren! Decoded sentence: Let's see that book.\n","\n","Input sentence: Vlucht. Decoded sentence: Let's see that book.\n","\n","Input sentence: Ren! Decoded sentence: Let's see that book.\n","\n","Input sentence: Vlucht. Decoded sentence: Let's see that book.\n","\n","Input sentence: Wie? Decoded sentence: Who is that wanter?\n","\n","Input sentence: Da's niet gek! Decoded sentence: That's a pine car.\n","\n","Input sentence: Eend! Decoded sentence: Are you strenge?\n","\n","Input sentence: Vuur! Decoded sentence: Let's see that book.\n","\n","Input sentence: Brand! Decoded sentence: Are you strenge?\n","\n","Input sentence: Help! Decoded sentence: My dog is stall.\n","\n","Input sentence: Spring. Decoded sentence: Let's see that book.\n","\n","Input sentence: Stop! Decoded sentence: Do you have a con?\n","\n","Input sentence: Hou op! Decoded sentence: My dog is stall.\n","\n","Input sentence: Hou daarmee op! Decoded sentence: My dog is stall.\n","\n","Input sentence: Geen beweging! Decoded sentence: Let's see that book.\n","\n"]}]},{"cell_type":"code","metadata":{"id":"Mg5JkTOaBSOu","executionInfo":{"status":"ok","timestamp":1637289507499,"user_tz":300,"elapsed":235,"user":{"displayName":"Oliver Posewitz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17090326966670826339"}}},"source":["\n","def text_encoder(my_phrase):\n","  if len(my_phrase) <= max_encoder_seq_length:\n","      # Fill up a single one-hot encoding vector with zeroes\n","      my_input_data =  np.zeros((1, max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\")\n","\n","      # Iterate over all of the characters in the input phrase\n","      for t, char in enumerate(my_phrase):\n","          my_input_data[0, t, input_token_index[char]] = 1.0\n","\n","      # This adds padding with spaces\n","      my_input_data[0, t + 1 :, input_token_index[\" \"]] = 1.0\n","\n","      return my_input_data\n","  else:\n","      print(\"Input phrase is longer than the maximum encoder sequence length.\")"],"execution_count":70,"outputs":[]},{"cell_type":"code","metadata":{"id":"11ZMSurMB-6r","executionInfo":{"status":"ok","timestamp":1637290435804,"user_tz":300,"elapsed":2751,"user":{"displayName":"Oliver Posewitz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17090326966670826339"}}},"source":["\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"iRZLSIngrOdP","executionInfo":{"status":"error","timestamp":1637288820661,"user_tz":300,"elapsed":7,"user":{"displayName":"Oliver Posewitz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17090326966670826339"}},"outputId":"3441c296-6fa7-4389-9b72-8bb38b91f295"},"source":["\n","# save list to txt\n","with open('/content/drive/MyDrive/NLP/hw5/english_reference_vector.txt', 'w') as f:\n","  for phrase in english_reference:\n","    f.write('%s\\n' % phrase)\n"],"execution_count":59,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-59-b6620884631b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# save list to txt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/NLP/hw5/english_reference_vector.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mphrase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menglish_reference\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mphrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/NLP/hw5/english_reference_vector.txt'"]}]},{"cell_type":"markdown","metadata":{"id":"RHYcEBFz9xMg"},"source":["# English to Spanish Model"]},{"cell_type":"code","metadata":{"id":"LoLIzln2qdoo"},"source":["# read in english to Spanish\n","# Download data\n","!curl -O http://www.manythings.org/anki/spa-eng.zip\n","!unzip spa-eng.zip\n","data_path_spa = \"spa.txt\" # Path to the data txt file on disk.\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AVtJ6QQK-fhk"},"source":["# Configuration\n","batch_size = 64\n","epochs = 15\n","latent_dim = 256\n","\n","# length of dataset\n","num_samples = 20000\n","\n","# Read in data\n","input_texts = []\n","target_texts = []\n","input_characters = set()\n","target_characters = set()\n","\n","# The *with* statement is a handy way of managing file I/O. It ensures that\n","# the file gets closed once we are done with it.\n","with open(data_path_spa, \"r\", encoding=\"utf-8\") as f:\n","    lines = f.read().split(\"\\n\")\n","\n","for line in lines[: min(num_samples, len(lines) - 1)]:\n","    target_text, input_text, _ = line.split(\"\\t\")\n","    # We use \"tab\" as the \"start sequence\" character\n","    # for the targets, and \"\\n\" as \"end sequence\" character.\n","    target_text = \"\\t\" + target_text + \"\\n\"\n","    input_texts.append(input_text)\n","    target_texts.append(target_text)\n","    for char in input_text:\n","        if char not in input_characters:\n","            input_characters.add(char)\n","    for char in target_text:\n","        if char not in target_characters:\n","            target_characters.add(char)\n","\n","print(len(input_texts), len(target_texts))\n","# Take a peek at the first few pairs of input and target texts\n","[ (i, t) for i, t in enumerate(zip(input_texts[:15], target_texts[:15]))]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NjZ-6YBR7vFr","executionInfo":{"status":"ok","timestamp":1637282606779,"user_tz":300,"elapsed":264,"user":{"displayName":"Oliver Posewitz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17090326966670826339"}}},"source":["# find the length of the maximum sequece for input and ouput\n","max_encoder_seq_length = max([len(txt) for txt in input_texts])\n","max_decoder_seq_length = max([len(txt) for txt in target_texts])\n","\n","print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n","print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n","\n","# sort list of unique characters for each input and get their lengths\n","input_characters = sorted(list(input_characters))\n","target_characters = sorted(list(target_characters))\n","num_encoder_tokens = len(input_characters)\n","num_decoder_tokens = len(target_characters)"],"execution_count":30,"outputs":[]}]}